{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YunDcnyU5pI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2498f68f-a19f-4795-9cad-fa038a20c0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QgJp_Sz852W8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import time\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvg0PTDCCh_b",
        "outputId": "d9b9a6bc-5d74-4804-c35e-af58dfba1bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random 50,000 lines written to 3.5_train.txt\n",
            "Random 2,500 lines written to 3.5_test.txt\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "# input_file = 'hugging_face_train.csv'\n",
        "train_output_file = '3.5_train.txt'\n",
        "test_output_file = '3.5_test.txt'\n",
        "\n",
        "# with open(input_file, 'r') as infile:\n",
        "#     lines = infile.readlines()\n",
        "\n",
        "# train_lines = random.sample(lines, 50000)\n",
        "\n",
        "# remaining_lines = list(set(lines) - set(train_lines))\n",
        "\n",
        "# test_lines = random.sample(remaining_lines, 2500)\n",
        "\n",
        "# with open(train_output_file, 'w') as train_file:\n",
        "#     train_file.writelines(train_lines)\n",
        "\n",
        "# with open(test_output_file, 'w') as test_file:\n",
        "#     test_file.writelines(test_lines)\n",
        "\n",
        "print(f\"Random 50,000 lines written to {train_output_file}\")\n",
        "print(f\"Random 2,500 lines written to {test_output_file}\")\n",
        "\n",
        "# # download\n",
        "# files.download(train_output_file)\n",
        "# files.download(test_output_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWpPVo0pIcKn",
        "outputId": "34fcdc00-17af-4392-ea1a-dc2270ffe7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DIKeKC9lkjV",
        "outputId": "50e01d9f-e4a6-4bcb-da5f-af3aacd19d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.70.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kQcG318endwD"
      },
      "outputs": [],
      "source": [
        "openai.api_key = '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' # can't leak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IyGNOgDMCIaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "outputId": "8eec9023-f47b-4da2-ea7e-716837155db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.18.3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "Invalid file format for Fine-Tuning API. Must be .jsonl",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-31db5a0bb4ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3.5_train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Uploaded file ID: {file_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-31db5a0bb4ac>\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         response = openai.File.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mpurpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fine-tune'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/file.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0muser_provided_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         )\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         return util.convert_to_openai_object(\n\u001b[1;32m     87\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid file format for Fine-Tuning API. Must be .jsonl"
          ]
        }
      ],
      "source": [
        "# Begin the training\n",
        "!pip install openai==0.28\n",
        "!pip install openai\n",
        "def upload_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = openai.File.create(\n",
        "            file=f,\n",
        "            purpose='fine-tune'\n",
        "        )\n",
        "    return response['id']\n",
        "\n",
        "dataset_path = '3.5_train.txt'\n",
        "file_id = upload_file(dataset_path)\n",
        "print(f\"Uploaded file ID: {file_id}\")\n",
        "\n",
        "def create_fine_tune_job(file_id, model='gpt-4o-mini'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "fine_tune_response = create_fine_tune_job(file_id)\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "import time\n",
        "\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTune.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(60)\n",
        "\n",
        "fine_tuned_model = openai.FineTune.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)\n",
        "\n",
        "print(\"done\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml-vKwbtn3WR",
        "outputId": "b4a99aa2-f524-4abd-ccb7-66ab7f631c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Using cached openai-1.70.0-py3-none-any.whl (599 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.70.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "Y5hNe9TurMAP",
        "outputId": "d9479e8e-8bf4-483c-c423-df12d25c1337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.70.0\n",
            "    Uninstalling openai-1.70.0:\n",
            "      Successfully uninstalled openai-1.70.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "2755b0c5f3284598b3e865d16f040955"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KQAXPKoMsVBX"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XddqpSZs-fUA",
        "outputId": "a47b422d-a0cf-461c-b70b-0bda8b429f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50000 lines to JSONL format and saved to formatted_dataset.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1246b15f-3de9-4710-825b-2f90ed1e4999\", \"formatted_dataset.jsonl\", 38348)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# had some formatting issues so trying jsonl\n",
        "\n",
        "import json\n",
        "\n",
        "input_file = '3.5_train.txt'\n",
        "output_file = 'formatted_dataset.jsonl'\n",
        "\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "json_lines = []\n",
        "for line in lines:\n",
        "\n",
        "    parts = line.strip().split(', ')\n",
        "    if len(parts) == 3:\n",
        "        clue, length, answer = parts\n",
        "        # Create a JSON object\n",
        "        json_obj = {\n",
        "            \"prompt\": f\"{clue}, {length}\",\n",
        "            \"completion\": answer.strip()\n",
        "        }\n",
        "        json_lines.append(json_obj)\n",
        "\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for json_obj in json_lines:\n",
        "        outfile.write(json.dumps(json_obj) + '\\n')\n",
        "\n",
        "print(f\"Converted {len(lines)} lines to JSONL format and saved to {output_file}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsKQ6YEDAthP",
        "outputId": "1f7055b2-5ce1-4f39-9d4e-02a2e89ce8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good\n"
          ]
        }
      ],
      "source": [
        "# check?\n",
        "import json\n",
        "\n",
        "output_file = 'formatted_dataset.jsonl'\n",
        "with open(output_file, 'r') as infile:\n",
        "    for line in infile:\n",
        "        try:\n",
        "            json_obj = json.loads(line)\n",
        "            if 'prompt' not in json_obj or 'completion' not in json_obj:\n",
        "                print(f\"Invalid format: {line}\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON decode error: {line}\")\n",
        "print(\"All good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfzRH40B_RCb",
        "outputId": "49758ee8-b64e-4b2d-8373-5dde4cf9c34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHI3ke3_sEj",
        "outputId": "a7aa6b37-48b3-44d8-e12e-bc7c0855fc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1VSHhZWAArCx",
        "outputId": "777ecdc8-838d-4e4f-8d44-24a02390a7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50000 lines to chat format and saved to chat_formatted_dataset.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ede1a14a-ab44-47a3-ba38-ca4a4d5e456a\", \"chat_formatted_dataset.jsonl\", 8200728)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "input_file = '3.5_train.txt'\n",
        "output_file = 'chat_formatted_dataset.jsonl'\n",
        "\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "json_lines = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split(',')\n",
        "    if len(parts) == 3:\n",
        "        clue, length, answer = parts\n",
        "        json_obj = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{clue}, {length}\"},\n",
        "                {\"role\": \"assistant\", \"content\": answer.strip()}\n",
        "            ]\n",
        "        }\n",
        "        json_lines.append(json_obj)\n",
        "\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for json_obj in json_lines:\n",
        "        outfile.write(json.dumps(json_obj) + '\\n')\n",
        "\n",
        "print(f\"Converted {len(lines)} lines to chat format and saved to {output_file}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oL66tq3e_7GA"
      },
      "outputs": [],
      "source": [
        "openai.api_key = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "78YumdBIGuFP",
        "outputId": "784e8bcd-6e2f-4e1a-d189-1c80c219df6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 2000 lines and saved to chat_formatted_dataset_2000.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bb8a301d-836e-4761-9383-47a6d1a2eb2b\", \"chat_formatted_dataset_2000.jsonl\", 373758)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# get lines\n",
        "import json\n",
        "import random\n",
        "\n",
        "input_file = 'chat_formatted_dataset.jsonl'\n",
        "output_file = 'chat_formatted_dataset_2000.jsonl'\n",
        "\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "if len(lines) < 350:\n",
        "    raise ValueError(\"The input file contains fewer than 2000 lines.\")\n",
        "\n",
        "selected_lines = random.sample(lines, 2000)\n",
        "\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for line in selected_lines:\n",
        "        outfile.write(line)\n",
        "\n",
        "print(f\"Selected 2000 lines and saved to {output_file}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCzwARI5nbBe",
        "outputId": "eb17800e-5653-43db-a8f9-de5dce2add44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded train file ID: file-JmdTgGfzCb9EsSEx6HLvYJ\n",
            "Fine-tune job created: ftjob-yVMfyi7ETxneda3EgztKdS13\n",
            "ftjob-yVMfyi7ETxneda3EgztKdS13\n",
            "Monitoring fine-tuning job...\n",
            "Job status: validating_files\n",
            "Job status: validating_files\n",
            "Job status: validating_files\n",
            "Job status: queued\n",
            "Job status: queued\n",
            "Job status: queued\n",
            "Job status: queued\n",
            "Job status: queued\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: succeeded\n",
            "Fine-tuning job completed successfully!\n",
            "Fine-tuned model name: ft:gpt-4o-mini-2024-07-18:inlp-project::BJcZyXEx\n"
          ]
        }
      ],
      "source": [
        "train_output_file = 'chat_formatted_dataset_2000.jsonl'\n",
        "\n",
        "def upload_file(file_path):\n",
        "    response = openai.File.create(\n",
        "        file=open(file_path, \"rb\"),\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "    return response['id']\n",
        "\n",
        "train_file_id = upload_file(train_output_file)\n",
        "print(f\"Uploaded train file ID: {train_file_id}\")\n",
        "\n",
        "def create_fine_tune_job(training_file_id, model='gpt-4o-mini-2024-07-18'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=training_file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Use the correct endpoint for creating fine-tune\n",
        "fine_tune_response = openai.FineTuningJob.create(training_file=train_file_id, model='gpt-4o-mini-2024-07-18')\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "def get_fine_tune_job_details(job_id):\n",
        "    response = openai.FineTuningJob.retrieve(id=job_id)\n",
        "    return response\n",
        "\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTuningJob.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "print(job_id)\n",
        "\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(45)\n",
        "\n",
        "fine_tuned_model = openai.FineTuningJob.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "seen_event_ids = set()\n",
        "print(\"Monitoring fine-tuning events...\\n\")\n",
        "\n",
        "with tqdm(desc=\"Fine-tuning\", unit=\"event\") as pbar:\n",
        "    while True:\n",
        "        status = openai.FineTuningJob.retrieve(id='ftjob-yVMfyi7ETxneda3EgztKdS13')['status']\n",
        "\n",
        "        # Get event logs\n",
        "        events = openai.FineTuningJob.list_events(id='ftjob-yVMfyi7ETxneda3EgztKdS13', limit=50)['data']\n",
        "        new_events = [e for e in events if e['id'] not in seen_event_ids]\n",
        "\n",
        "        for event in reversed(new_events):\n",
        "            message = event['message']\n",
        "            created_at = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(event['created_at']))\n",
        "            print(f\"[{created_at}] {message}\")\n",
        "            seen_event_ids.add(event['id'])\n",
        "            pbar.update(1)\n",
        "\n",
        "        if status in ['succeeded', 'failed']:\n",
        "            break\n",
        "        time.sleep(30)\n",
        "\n",
        "# Get the fine-tuned model name\n",
        "fine_tuned_model = openai.FineTuningJob.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"\\n🎉 Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGRyIcbrctYu",
        "outputId": "3891fc7f-f7f8-4dc7-8f7d-2f36b87eb840"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monitoring fine-tuning events...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tuning: 50event [00:00, 147.64event/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-07 08:56:20] Step 1455/1500: training loss=0.83\n",
            "[2025-04-07 08:56:22] Step 1456/1500: training loss=0.23\n",
            "[2025-04-07 08:56:22] Step 1457/1500: training loss=0.19\n",
            "[2025-04-07 08:56:22] Step 1458/1500: training loss=0.41\n",
            "[2025-04-07 08:56:25] Step 1459/1500: training loss=0.46\n",
            "[2025-04-07 08:56:25] Step 1460/1500: training loss=0.54\n",
            "[2025-04-07 08:56:25] Step 1461/1500: training loss=0.68\n",
            "[2025-04-07 08:56:28] Step 1462/1500: training loss=1.73\n",
            "[2025-04-07 08:56:28] Step 1463/1500: training loss=0.49\n",
            "[2025-04-07 08:56:28] Step 1464/1500: training loss=0.29\n",
            "[2025-04-07 08:56:31] Step 1465/1500: training loss=0.04\n",
            "[2025-04-07 08:56:31] Step 1466/1500: training loss=0.38\n",
            "[2025-04-07 08:56:31] Step 1467/1500: training loss=0.80\n",
            "[2025-04-07 08:56:34] Step 1468/1500: training loss=0.75\n",
            "[2025-04-07 08:56:34] Step 1469/1500: training loss=0.45\n",
            "[2025-04-07 08:56:34] Step 1470/1500: training loss=0.40\n",
            "[2025-04-07 08:56:36] Step 1471/1500: training loss=0.14\n",
            "[2025-04-07 08:56:37] Step 1472/1500: training loss=0.75\n",
            "[2025-04-07 08:56:37] Step 1473/1500: training loss=0.34\n",
            "[2025-04-07 08:56:39] Step 1474/1500: training loss=0.32\n",
            "[2025-04-07 08:56:39] Step 1475/1500: training loss=0.30\n",
            "[2025-04-07 08:56:39] Step 1476/1500: training loss=0.25\n",
            "[2025-04-07 08:56:42] Step 1477/1500: training loss=1.23\n",
            "[2025-04-07 08:56:42] Step 1478/1500: training loss=0.32\n",
            "[2025-04-07 08:56:42] Step 1479/1500: training loss=0.32\n",
            "[2025-04-07 08:56:45] Step 1480/1500: training loss=0.48\n",
            "[2025-04-07 08:56:45] Step 1481/1500: training loss=0.21\n",
            "[2025-04-07 08:56:45] Step 1482/1500: training loss=0.11\n",
            "[2025-04-07 08:56:48] Step 1483/1500: training loss=0.38\n",
            "[2025-04-07 08:56:48] Step 1484/1500: training loss=0.36\n",
            "[2025-04-07 08:56:48] Step 1485/1500: training loss=0.16\n",
            "[2025-04-07 08:56:50] Step 1486/1500: training loss=0.16\n",
            "[2025-04-07 08:56:50] Step 1487/1500: training loss=0.39\n",
            "[2025-04-07 08:56:50] Step 1488/1500: training loss=0.86\n",
            "[2025-04-07 08:56:53] Step 1489/1500: training loss=0.20\n",
            "[2025-04-07 08:56:53] Step 1490/1500: training loss=0.19\n",
            "[2025-04-07 08:56:53] Step 1491/1500: training loss=0.02\n",
            "[2025-04-07 08:56:56] Step 1492/1500: training loss=0.33\n",
            "[2025-04-07 08:56:56] Step 1493/1500: training loss=0.22\n",
            "[2025-04-07 08:56:56] Step 1494/1500: training loss=0.78\n",
            "[2025-04-07 08:56:59] Step 1495/1500: training loss=0.05\n",
            "[2025-04-07 08:56:59] Step 1496/1500: training loss=0.55\n",
            "[2025-04-07 08:56:59] Step 1497/1500: training loss=0.08\n",
            "[2025-04-07 08:57:02] Step 1498/1500: training loss=1.45\n",
            "[2025-04-07 08:57:02] Step 1499/1500: training loss=0.64\n",
            "[2025-04-07 08:57:02] Step 1500/1500: training loss=0.01\n",
            "[2025-04-07 08:57:10] Checkpoint created at step 500\n",
            "[2025-04-07 08:57:10] Checkpoint created at step 1000\n",
            "[2025-04-07 08:57:11] New fine-tuned model created\n",
            "[2025-04-07 08:57:17] The job has successfully completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 Fine-tuned model name: ft:gpt-4o-mini-2024-07-18:inlp-project::BJcZyXEx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P54ycfv3e91o",
        "outputId": "adeee25a-602f-45f7-c3ab-c25384c8909f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "openai.api_key = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt-BFSYtfHoC",
        "outputId": "5e50452b-3214-4121-9f6d-314f933b7e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W7VYh9uIgnu9"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'r') as f:\n",
        "    fine_tuned_model = f.read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSGDzJZmf4Yl",
        "outputId": "91a76c93-d49b-4ab8-8e0b-ade8d39ef22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1: anise\n",
            "Completion 2: anise\n",
            "Completion 3: anise\n",
            "Completion 4: anise\n",
            "Completion 5: herb\n",
            "Using fine-tuned model: ft:gpt-4o-mini-2024-07-18:inlp-project::BJcZyXEx\n",
            "Prompt: Some crumbly blocks, 4\n",
            "Completion: feta\n"
          ]
        }
      ],
      "source": [
        "# def generate_completion(prompt, model):\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#         model=model,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt}\n",
        "#         ],\n",
        "#         max_tokens=50\n",
        "#     )\n",
        "#     return response.choices[0].message['content'].strip()\n",
        "\n",
        "\n",
        "# def generate_multiple_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "#     responses = openai.ChatCompletion.create(\n",
        "#         model=model,\n",
        "#         prompt=prompt,\n",
        "#         max_tokens=max_tokens,\n",
        "#         n=num_completions,\n",
        "#         stop=None,\n",
        "#         temperature=0.7\n",
        "#     )\n",
        "#     completions = [response['text'].strip() for response in responses['choices']]\n",
        "#     return completions\n",
        "\n",
        "# prompt = \"Fennel or sweet cicely\"\n",
        "# completions = generate_multiple_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "# for i, completion in enumerate(completions, 1):\n",
        "#     print(f\"Completion {i}: {completion}\")\n",
        "\n",
        "# print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "# prompt = \"Some crumbly blocks, 4\"\n",
        "# completion = generate_completion(prompt, fine_tuned_model)\n",
        "# print(f\"Prompt: {prompt}\")\n",
        "# print(f\"Completion: {completion}\")\n",
        "\n",
        "import openai\n",
        "\n",
        "def generate_completion(prompt, model):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def generate_multiple_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    completions = [choice['message']['content'].strip() for choice in response['choices']]\n",
        "    return completions\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completions = generate_multiple_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n",
        "\n",
        "print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "prompt = \"Some crumbly blocks, 4\"\n",
        "completion = generate_completion(prompt, fine_tuned_model)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Completion: {completion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S_0DU7ChH_7",
        "outputId": "bc1e46ec-cace-4926-d1da-65ac838de7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1: reads\n",
            "Completion 2: auditions\n",
            "Completion 3: REARS\n",
            "Completion 4: reels\n"
          ]
        }
      ],
      "source": [
        "# use this function because it works\n",
        "\n",
        "import openai\n",
        "\n",
        "def generate_unique_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions * 2,\n",
        "        stop=None,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    completions = set()\n",
        "    for choice in response['choices']:\n",
        "        completions.add(choice['message']['content'].strip())\n",
        "        if len(completions) >= num_completions:\n",
        "            break\n",
        "\n",
        "    return list(completions)[:num_completions]\n",
        "\n",
        "prompt = \"Tries for a role, 5, R___S\"\n",
        "completions = generate_unique_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wipz6_tXi8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcrxYKNmQne"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "srlGWNpRl0iD",
        "outputId": "0f220be1-b8ad-4fc2-c86c-7310b4a5ec21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file ID: file-tXI1N4xwQgDm6wDJZAE6TQie\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_file_id' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-99b17153746d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfine_tune_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_fine_tune_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fine-tune job created: {fine_tune_response['id']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_file_id' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "\n",
        "def upload_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = openai.File.create(\n",
        "            file=f,\n",
        "            purpose='fine-tune'\n",
        "        )\n",
        "    return response['id']\n",
        "\n",
        "dataset_path = 'gpt_3.5_train.txt'\n",
        "file_id = upload_file(dataset_path)\n",
        "print(f\"Uploaded file ID: {file_id}\")\n",
        "\n",
        "def create_fine_tune_job(file_id, model='gpt-3.5-turbo'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "fine_tune_response = create_fine_tune_job(train_file_id)\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTune.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(60)\n",
        "\n",
        "fine_tuned_model = openai.FineTune.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdDyxnt7lQll"
      },
      "outputs": [],
      "source": [
        "# Potential Load???? Figure this out?\n",
        "\n",
        "#edit: this works\n",
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'r') as f:\n",
        "    fine_tuned_model = f.read().strip()\n",
        "\n",
        "def generate_completion(prompt, model):\n",
        "    response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completion = generate_completion(prompt, fine_tuned_model)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Completion: {completion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x29D6Xvjitrb"
      },
      "outputs": [],
      "source": [
        "def generate_multiple_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    responses = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    completions = [response['text'].strip() for response in responses['choices']]\n",
        "    return completions\n",
        "\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completions = generate_multiple_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "P3F9bIFYGg7y",
        "outputId": "598184d2-4623-42d1-9df7-ed95d11ee3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random 50,000 lines written to gpt_3.5_train.txt\n",
            "Random 2,500 lines written to gpt_3.5_test.txt\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4fba1d17-9038-47d6-b1f3-8d44bcb0621c\", \"gpt_3.5_train.txt\", 1459214)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_38a6c34a-c1d3-4261-a9ea-0843980c0adb\", \"gpt_3.5_test.txt\", 77936)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "input_file = 'reformatted.txt'\n",
        "train_output_file = 'gpt_3.5_train.txt'\n",
        "test_output_file = 'gpt_3.5_test.txt'\n",
        "\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "train_lines = random.sample(lines, 50000)\n",
        "\n",
        "remaining_lines = list(set(lines) - set(train_lines))\n",
        "\n",
        "test_lines = random.sample(remaining_lines, 2500)\n",
        "\n",
        "with open(train_output_file, 'w') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "with open(test_output_file, 'w') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "print(f\"Random 50,000 lines written to {train_output_file}\")\n",
        "print(f\"Random 2,500 lines written to {test_output_file}\")\n",
        "\n",
        "files.download(train_output_file)\n",
        "files.download(test_output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}